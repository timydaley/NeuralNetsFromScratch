{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutions neural nets from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this document is to document my attempt to build and apply neural networks to DNA sequence data, specifically to sgRNA screening data.  The motivation of this document is Andrej Karpathy's Hacker's guide to Neural Networks. Like his work we will build our models up, starting from a one layer network on up.  The hope is that I can gain a better understanding of building deep networks and backpropagation by working my way up.  \n",
    "\n",
    "In our data we have a 40 base pair sequence (20 bp of the sgRNA + 3 bp PAM + 10 bp upstream + 10 bp downstream - 2 bp (NGG PAM) - 1 bp from G in first position).  We'll pass each of the 4 bases in each position to a single neuron (40 in the first layer).  For each neuron $i$ in the first layer, we'll compute a score $s_{i} = a_{i} 1(A) + c_{i} 1(C) + g_{i} 1(G) + t_{i} 1(T)$ and then apply ReLU transformation (relu(x) = $\\max(0, x)$).  The transformed score is then passed to a regression model, i.e. \n",
    "$$\n",
    "\\hat{y}_{i} = \\sum_{i = 1}^{40} \\beta_{i} \\text{relu}(s_{i}).\n",
    "$$\n",
    "We'll use square loss from the observed data to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuron:\n",
    "    def __init__(self, a, c, g, t):\n",
    "        self.a = a\n",
    "        self.c = c\n",
    "        self.g = g\n",
    "        self.t = t\n",
    "        self.grad = [0.0]*4\n",
    "    def set_params(self, a, c, g, t):\n",
    "        self.a = a\n",
    "        self.c = c\n",
    "        self.g = g\n",
    "        self.t = t\n",
    "    def get_params(self):\n",
    "        return [self.a, self.c, self.g, self.t]\n",
    "    def compute_score(self, A, C, G, T):\n",
    "        self.score = max(self.a*A + self.c*C + self.g*G + self.t*T, 0.0)\n",
    "        return self.score\n",
    "    def compute_gradient(self, A, C, G, T):\n",
    "        if self.score < 0:\n",
    "            self.grad = [0.0]*4\n",
    "        else:\n",
    "            self.grad = [A, C, G, T]\n",
    "    def backward(self, A, C, G, T, prev_grad):\n",
    "        self.compute_gradient(A, C, G, T)\n",
    "        self.grad = numpy.multiply(self.grad, prev_grad)\n",
    "\n",
    "\n",
    "class regression_gate:\n",
    "    def __init__(self, n_params):\n",
    "        self.n_params = n_params\n",
    "        r = numpy.random.normal(0, 1, n_params)\n",
    "        self.params = r/sum(r)\n",
    "        self.grad = [0.0]*n_params\n",
    "    def set_params(self, params):\n",
    "        # make sure the lengths are correct\n",
    "        assert len(params) == self.n_params, \"wrong input length\"\n",
    "        self.params = params\n",
    "    def compute_score(self, x):\n",
    "        assert len(x) == self.n_params, \"wrong input length\"\n",
    "        ret = 0.0\n",
    "        for i in range(self.n_params):\n",
    "            ret += self.params[i]*x[i]\n",
    "        self.score = ret\n",
    "        return ret\n",
    "    def compute_gradient(self, x):\n",
    "        self.grad = x\n",
    "    def backward(self, x, prev_grad):\n",
    "        self.compute_gradient(x)\n",
    "        self.grad = numpy.multiply(x, prev_grad)\n",
    "        \n",
    "class logistic_regression_gate:\n",
    "    def __init__(self, n_params):\n",
    "        self.n_params = n_params\n",
    "        self.params = numpy.random.normal(0, 1, n_params)\n",
    "        self.grad = [0.0]*n_params\n",
    "    def set_params(self, params):\n",
    "        # make sure the lengths are correct\n",
    "        assert len(params) == self.n_params, \"wrong input length\"\n",
    "        self.params = params\n",
    "    def compute_score(self, x):\n",
    "        assert len(x) == self.n_params, \"wrong input length\"\n",
    "        ret = 0.0\n",
    "        for i in range(self.n_params):\n",
    "            ret += self.params[i]*x[i]\n",
    "        self.score = 1 / (1 + math.exp(-ret))\n",
    "        return self.score\n",
    "    def compute_gradient(self, x):\n",
    "        self.compute_score(x)\n",
    "        self.grad = self.score*(1 - self.score)*x\n",
    "    def backward(self, x, prev_grad):\n",
    "        self.compute_gradient(x)\n",
    "        self.grad = numpy.multiply(x, prev_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2073, 163)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NeuronTopGeneGuidesMixtureProbs = pandas.read_csv(\"~/sgRNA/tiling/deepLearningMixtureRegression/NeuronTopGeneGuidesMixtureProbs.txt\", sep='\\t', header=0)\n",
    "SelfRenewalTopGeneGuidesMixtureProbs = pandas.read_csv(\"~/sgRNA/tiling/deepLearningMixtureRegression/SelfRenewalTopGeneGuidesMixtureProbs.txt\", sep='\\t', header=0)\n",
    "combinedGuides = pandas.concat([NeuronTopGeneGuidesMixtureProbs, SelfRenewalTopGeneGuidesMixtureProbs], ignore_index=True)\n",
    "\n",
    "splitSeqs = []\n",
    "for i in range(combinedGuides.shape[0]):\n",
    "    splitSeqs = splitSeqs + list(str(combinedGuides[\"seq\"][i]))\n",
    "\n",
    "splitSeqsDF = pandas.DataFrame(numpy.array(splitSeqs).reshape(combinedGuides.shape[0], 43))\n",
    "splitSeqsDummies = pandas.get_dummies(splitSeqsDF)\n",
    "splitSeqsDummies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2073, 160)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitSeqsDummies.drop([col for col, val in splitSeqsDummies.sum().iteritems() if val == 2073], axis=1, inplace=True)\n",
    "splitSeqsDummies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1658, 161)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = splitSeqsDummies\n",
    "dataset[\"y\"] = combinedGuides[\"mixture_probs\"]\n",
    "\n",
    "train_dataset = dataset.sample(frac=0.8,random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_outcome = train_dataset.pop('y')\n",
    "test_outcome = test_dataset.pop('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  0\n",
      "iteration  500\n",
      "iteration  1000\n",
      "iteration  1500\n",
      "iteration  2000\n",
      "iteration  2500\n",
      "iteration  3000\n",
      "iteration  3500\n",
      "iteration  4000\n",
      "iteration  4500\n",
      "iteration  5000\n",
      "iteration  5500\n",
      "iteration  6000\n",
      "iteration  6500\n",
      "iteration  7000\n",
      "iteration  7500\n",
      "iteration  8000\n",
      "iteration  8500\n",
      "iteration  9000\n",
      "iteration  9500\n",
      "iteration  10000\n",
      "iteration  10500\n",
      "iteration  11000\n",
      "iteration  11500\n",
      "iteration  12000\n",
      "iteration  12500\n",
      "iteration  13000\n",
      "iteration  13500\n",
      "iteration  14000\n",
      "iteration  14500\n",
      "iteration  15000\n",
      "iteration  15500\n",
      "iteration  16000\n",
      "iteration  16500\n",
      "iteration  17000\n",
      "iteration  17500\n",
      "iteration  18000\n",
      "iteration  18500\n",
      "iteration  19000\n",
      "iteration  19500\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "neurons = []\n",
    "for i in range(40):\n",
    "    r = numpy.random.normal(0, 1, 4)\n",
    "    n = neuron(a = r[0], c = r[1], g = r[2], t = r[3])\n",
    "    neurons.append(n)\n",
    "    \n",
    "pooled_regression = regression_gate(41)\n",
    "\n",
    "iter = 20000\n",
    "epsilon = 5e-2\n",
    "mse_train = []\n",
    "mse_test = []\n",
    "n_bases = 40\n",
    "\n",
    "# sgd\n",
    "for i in range(iter):\n",
    "    if i % 500 == 0:\n",
    "        print(\"iteration \", i)\n",
    "    step_size = epsilon/math.sqrt(i + 1)\n",
    "    indx = random.randint(0, len(train_outcome) - 1)\n",
    "    x = train_dataset.iloc[indx]\n",
    "    y = train_outcome.iloc[indx]\n",
    "    # compute regression with current coefficients\n",
    "    neuron_vals = [1.0] # first val is constant\n",
    "    for j in range(n_bases):\n",
    "        A = x.iloc[4*j]\n",
    "        C = x.iloc[4*j + 1]\n",
    "        G = x.iloc[4*j + 2]\n",
    "        T = x.iloc[4*j + 3]\n",
    "        neuron_vals.append(neurons[j].compute_score(A, C, G, T))\n",
    "    #print(\"neuron vals = \", neuron_vals)\n",
    "    prev_score = pooled_regression.compute_score(neuron_vals)\n",
    "    #print(\"prev score = \", prev_score)\n",
    "    #print(\"current val = \", y)\n",
    "    #print(\"prev_score - y = \", prev_score - y)\n",
    "    prev_diff = prev_score - y\n",
    "    # compute gradient by backprop\n",
    "    prev_grad = [prev_score - y]*41\n",
    "    pooled_regression.backward(neuron_vals, prev_grad)\n",
    "    #print(\"pooled_regression.params = \", pooled_regression.params)\n",
    "    #print(\"pooled_regression.grad = \", pooled_regression.grad)\n",
    "    for j in range(40):\n",
    "        A = x.iloc[4*j]\n",
    "        C = x.iloc[4*j + 1]\n",
    "        G = x.iloc[4*j + 2]\n",
    "        T = x.iloc[4*j + 3]\n",
    "        prev_grad = pooled_regression.grad[j + 1]\n",
    "        neurons[j].backward(A, C, G, T, prev_grad)\n",
    "    # update params\n",
    "    pooled_regression.set_params(pooled_regression.params - step_size*pooled_regression.grad)\n",
    "    for j in range(40):\n",
    "        grad = neurons[j].grad\n",
    "        neurons[j].set_params(neurons[j].a - step_size*grad[0], \n",
    "                              neurons[j].c - step_size*grad[1], \n",
    "                              neurons[j].g - step_size*grad[2], \n",
    "                              neurons[j].t - step_size*grad[3])\n",
    "    neuron_vals = [1.0] # first val is constant\n",
    "    for j in range(40):\n",
    "        A = x.iloc[4*j]\n",
    "        C = x.iloc[4*j + 1]\n",
    "        G = x.iloc[4*j + 2]\n",
    "        T = x.iloc[4*j + 3]\n",
    "        neuron_vals.append(neurons[j].compute_score(A, C, G, T))\n",
    "    current_score = pooled_regression.compute_score(neuron_vals)\n",
    "    curr_diff = current_score - y\n",
    "    if abs(prev_diff) < abs(curr_diff):\n",
    "        print(\"score not improved!\")\n",
    "    #print(\"current score = \", current_score)\n",
    "    if i % 10 == 0:\n",
    "        e = 0.0\n",
    "        for k in range(len(train_outcome)):\n",
    "            neuron_vals = [1.0] # first val is constant\n",
    "            x = train_dataset.iloc[k] \n",
    "            for j in range(40):\n",
    "                A = x.iloc[4*j]\n",
    "                C = x.iloc[4*j + 1]\n",
    "                G = x.iloc[4*j + 2]\n",
    "                T = x.iloc[4*j + 3]\n",
    "                neuron_vals.append(neurons[j].compute_score(A, C, G, T))\n",
    "            y = train_outcome.iloc[k]\n",
    "            y_hat = pooled_regression.compute_score(neuron_vals)\n",
    "            e += numpy.multiply(y - y_hat, y - y_hat)\n",
    "        mse_train.append(e/len(train_outcome))\n",
    "        e = 0.0\n",
    "        for k in range(len(test_outcome)):\n",
    "            neuron_vals = [1.0] # first val is constant\n",
    "            x = test_dataset.iloc[k] \n",
    "            for j in range(40):\n",
    "                A = x.iloc[4*j]\n",
    "                C = x.iloc[4*j + 1]\n",
    "                G = x.iloc[4*j + 2]\n",
    "                T = x.iloc[4*j + 3]\n",
    "                neuron_vals.append(neurons[j].compute_score(A, C, G, T))\n",
    "            y = test_outcome.iloc[k]\n",
    "            y_hat = pooled_regression.compute_score(neuron_vals)\n",
    "            e += numpy.multiply(y - y_hat, y - y_hat)\n",
    "        mse_test.append(e/len(test_outcome)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([int(x)*10 for x in range(len(mse_train))], mse_train)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([int(x)*10 for x in range(len(mse_test))], mse_test)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('test MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works!  We can quantify each base's contribution to the prediction by looking at the product of the regression coefficient for that base and then multplying that by the base's coefficient in the neuron.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_effects = numpy.zeros((4, 40))\n",
    "for b in range(base_effects.shape[0]):\n",
    "    for i in range(base_effects.shape[1]):        \n",
    "        base_effects[b, i] = neurons[i].get_params()[b]*pooled_regression.params[i + 1]\n",
    "plt.pcolor(base_effects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That color scheme is honestly terrible, and we need a legend to tell what color correpsonds to what."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "seaborn.heatmap(base_effects, cmap=\"RdBu_r\", yticklabels = ['A', 'C', 'G', 'T'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this to the estimates from a mixture-based logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='/Users/Daley/sgRNA/tiling/metaAnalysis/MetaMixCistromeSequenceFeatures.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
