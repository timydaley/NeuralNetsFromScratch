{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutions neural nets from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this document is to document my attempt to build and apply neural networks to DNA sequence data, specifically to sgRNA screening data.  The motivation of this document is Andrej Karpathy's Hacker's guide to Neural Networks. Like his work we will build our models up, starting from a one layer network on up.  The hope is that I can gain a better understanding of building deep networks and backpropagation by working my way up.  \n",
    "\n",
    "In our data we have a 40 base pair sequence (20 bp of the sgRNA + 3 bp PAM + 10 bp upstream + 10 bp downstream - 2 bp (NGG PAM) - 1 bp from G in first position).  We'll pass each of the 4 bases in each position to a single neuron (40 in the first layer).  For each neuron $i$ in the first layer, we'll compute a score $s_{i} = a_{i} 1(A) + c_{i} 1(C) + g_{i} 1(G) + t_{i} 1(T)$ and then apply ReLU transformation (relu(x) = $\\max(0, x)$).  The transformed score is then passed to a regression model, i.e. \n",
    "$$\n",
    "\\hat{y}_{i} = \\sum_{i = 1}^{40} \\beta_{i} \\text{relu}(s_{i}).\n",
    "$$\n",
    "We'll use square loss from the observed data to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def relu(x):\n",
    "    return max(0.0, x)\n",
    "\n",
    "def square_loss(y, y_hat):\n",
    "    return sum((y - y_hat)*(y - y_hat))\n",
    "\n",
    "class neuron:\n",
    "    def __init__():\n",
    "        # set initial values to random standard normal variates\n",
    "        self.a = numpy.random.normal(0, 1, 1)\n",
    "        self.c = numpy.random.normal(0, 1, 1)\n",
    "        self.g = numpy.random.normal(0, 1, 1)\n",
    "        self.t = numpy.random.normal(0, 1, 1)\n",
    "    def __init__(a, c, g, t):\n",
    "        self.a = a\n",
    "        self.c = c\n",
    "        self.g = g\n",
    "        self.t = t        \n",
    "    def compute_score(A, C, G, T):\n",
    "        self.score = relu(self.a*A + self.c*C + self.g*G + self.t*T)\n",
    "    def gradient(A, C, G, T):\n",
    "        if self.score < 0:\n",
    "            self.grad = [0, 0, 0, 0]\n",
    "        else:\n",
    "            self.grad = [A, C, G, T]\n",
    "    def update_grad(epsilon, A, C, G, T):\n",
    "        self.a += epsilon*grad[0]\n",
    "        self.c += epsilon*grad[1]\n",
    "        self.g += epsilon*grad[2]\n",
    "        self.t += epsilon*grad[3]\n",
    "\n",
    "\n",
    "class regression_gate:\n",
    "    def __init__(n_params):\n",
    "        self.n_params = n_params\n",
    "        self.params = [0.0]*n_params\n",
    "    def set_params(params):\n",
    "        # make sure the lengths are correct\n",
    "        assert len(params) == self.n_params, \"wrong input length\"\n",
    "        self.params = params\n",
    "    def compute_score(x):\n",
    "        assert len(x) == self.n_params, \"wrong input length\"\n",
    "        ret = 0.0\n",
    "        for i in range(self.n_params):\n",
    "            ret += self.params[i]*x[i]\n",
    "        return ret\n",
    "    def gradient(x):\n",
    "        return x\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
