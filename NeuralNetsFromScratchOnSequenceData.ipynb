{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutions neural nets from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this document is to document my attempt to build and apply neural networks to DNA sequence data, specifically to sgRNA screening data.  The motivation of this document is Andrej Karpathy's Hacker's guide to Neural Networks. Like his work we will build our models up, starting from a one layer network on up.  The hope is that I can gain a better understanding of building deep networks and backpropagation by working my way up.  \n",
    "\n",
    "In our data we have a 40 base pair sequence (20 bp of the sgRNA + 3 bp PAM + 10 bp upstream + 10 bp downstream - 2 bp (NGG PAM) - 1 bp from G in first position).  We'll pass each of the 4 bases in each position to a single neuron (40 in the first layer).  For each neuron $i$ in the first layer, we'll compute a score $s_{i} = a_{i} 1(A) + c_{i} 1(C) + g_{i} 1(G) + t_{i} 1(T)$ and then apply ReLU transformation (relu(x) = $\\max(0, x)$).  The transformed score is then passed to a regression model, i.e. \n",
    "$$\n",
    "\\hat{y}_{i} = \\sum_{i = 1}^{40} \\beta_{i} \\text{relu}(s_{i}).\n",
    "$$\n",
    "We'll use square loss from the observed data to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2073, 163)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def relu(x):\n",
    "    return max(0.0, x)\n",
    "\n",
    "def square_loss(y, y_hat):\n",
    "    return sum((y - y_hat)*(y - y_hat))\n",
    "\n",
    "class neuron:\n",
    "    def __init__(self, a, c, g, t):\n",
    "        self.a = a\n",
    "        self.c = c\n",
    "        self.g = g\n",
    "        self.t = t\n",
    "        self.grad = [0.0]*4\n",
    "    def set_params(self, a, c, g, t):\n",
    "        self.a = a\n",
    "        self.c = c\n",
    "        self.g = g\n",
    "        self.t = t\n",
    "    def compute_score(self, A, C, G, T):\n",
    "        self.score = relu(self.a*A + self.c*C + self.g*G + self.t*T)\n",
    "        return self.score\n",
    "    def gradient(self, A, C, G, T):\n",
    "        if self.score < 0:\n",
    "            self.grad = [0.0]*4\n",
    "        else:\n",
    "            self.grad = [A, C, G, T]\n",
    "    def backward(self, A, C, G, T, prev_grad):\n",
    "        self.gradient(A, C, G, T)\n",
    "        self.grad = numpy.multiply(self.grad, prev_grad)\n",
    "\n",
    "\n",
    "class regression_gate:\n",
    "    def __init__(self, n_params):\n",
    "        self.n_params = n_params\n",
    "        self.params = numpy.random.normal(0, 1, n_params)\n",
    "        self.grad = [0.0]*n_params\n",
    "    def set_params(self, params):\n",
    "        # make sure the lengths are correct\n",
    "        assert len(params) == self.n_params, \"wrong input length\"\n",
    "        self.params = params\n",
    "    def compute_score(self, x):\n",
    "        assert len(x) == self.n_params, \"wrong input length\"\n",
    "        ret = 0.0\n",
    "        for i in range(self.n_params):\n",
    "            ret += self.params[i]*x[i]\n",
    "        self.score = ret\n",
    "        return ret\n",
    "    def gradient(self, x):\n",
    "        self.grad = x\n",
    "    def backward(self, x, prev_grad):\n",
    "        self.grad = numpy.multiply(x, prev_grad)\n",
    "\n",
    "\n",
    "NeuronTopGeneGuidesMixtureProbs = pandas.read_csv(\"~/sgRNA/tiling/deepLearningMixtureRegression/NeuronTopGeneGuidesMixtureProbs.txt\", sep='\\t', header=0)\n",
    "SelfRenewalTopGeneGuidesMixtureProbs = pandas.read_csv(\"~/sgRNA/tiling/deepLearningMixtureRegression/SelfRenewalTopGeneGuidesMixtureProbs.txt\", sep='\\t', header=0)\n",
    "combinedGuides = pandas.concat([NeuronTopGeneGuidesMixtureProbs, SelfRenewalTopGeneGuidesMixtureProbs], ignore_index=True)\n",
    "\n",
    "splitSeqs = []\n",
    "for i in range(combinedGuides.shape[0]):\n",
    "    splitSeqs = splitSeqs + list(str(combinedGuides[\"seq\"][i]))\n",
    "\n",
    "splitSeqsDF = pandas.DataFrame(numpy.array(splitSeqs).reshape(combinedGuides.shape[0], 43))\n",
    "splitSeqsDummies = pandas.get_dummies(splitSeqsDF)\n",
    "splitSeqsDummies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2073, 160)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitSeqsDummies.drop([col for col, val in splitSeqsDummies.sum().iteritems() if val == 2073], axis=1, inplace=True)\n",
    "splitSeqsDummies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1658, 161)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = splitSeqsDummies\n",
    "dataset[\"y\"] = combinedGuides[\"mixture_probs\"]\n",
    "\n",
    "train_dataset = dataset.sample(frac=0.8,random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_outcome = train_dataset.pop('y')\n",
    "test_outcome = test_dataset.pop('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11273194136973472\n",
      "0.11290259624422008\n",
      "9.276221545624402\n",
      "9.276499429068942\n",
      "6.14679755497875\n",
      "6.147070369082537\n",
      "5.572294587330601\n",
      "5.572479587439426\n",
      "-0.869976354267278\n",
      "-0.8698813950597142\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "neurons = []\n",
    "for i in range(40):\n",
    "    r = numpy.random.normal(0, 1, 4)\n",
    "    n = neuron(a = r[0], c = r[1], g = r[2], t = r[3])\n",
    "    neurons.append(n)\n",
    "    \n",
    "pooled_regression = regression_gate(41)\n",
    "\n",
    "iter = 5\n",
    "epsilon = 1.0e-5\n",
    "mse_train = []\n",
    "mse_test = []\n",
    "\n",
    "# sgd\n",
    "for i in range(iter):\n",
    "    indx = random.randint(0, len(train_outcome) - 1)\n",
    "    x = train_dataset.iloc[indx]\n",
    "    y = train_outcome.iloc[indx]\n",
    "    # compute regression with current coefficients\n",
    "    neuron_vals = [1.0] # first val is constant\n",
    "    for j in range(40):\n",
    "        A = x.iloc[4*j]\n",
    "        C = x.iloc[4*j + 1]\n",
    "        G = x.iloc[4*j + 2]\n",
    "        T = x.iloc[4*j + 3]\n",
    "        neuron_vals.append(neurons[j].compute_score(A, C, G, T))\n",
    "    prev_score = pooled_regression.compute_score(neuron_vals)\n",
    "    print(prev_score)\n",
    "    # compute gradient by backprop\n",
    "    prev_grad = [1.0]*41\n",
    "    pooled_regression.backward(neuron_vals, prev_grad)\n",
    "    for j in range(40):\n",
    "        prev_grad = pooled_regression.grad[j + 1]\n",
    "        A = x.iloc[4*j]\n",
    "        C = x.iloc[4*j + 1]\n",
    "        G = x.iloc[4*j + 2]\n",
    "        T = x.iloc[4*j + 3]\n",
    "        neurons[j].backward(A, C, G, T, prev_grad)\n",
    "    # update params\n",
    "    pooled_regression.set_params(pooled_regression.params + epsilon*pooled_regression.grad)\n",
    "    for j in range(40):\n",
    "        grad = neurons[j].grad\n",
    "        neurons[j].set_params(neurons[j].a + epsilon*grad[0], \n",
    "                              neurons[j].c + epsilon*grad[1], \n",
    "                              neurons[j].g + epsilon*grad[2], \n",
    "                              neurons[j].t + epsilon*grad[3])\n",
    "    neuron_vals = [1.0] # first val is constant\n",
    "    for j in range(40):\n",
    "        A = x.iloc[4*j]\n",
    "        C = x.iloc[4*j + 1]\n",
    "        G = x.iloc[4*j + 2]\n",
    "        T = x.iloc[4*j + 3]\n",
    "        neuron_vals.append(neurons[j].compute_score(A, C, G, T))\n",
    "    current_score = pooled_regression.compute_score(neuron_vals)\n",
    "    print(current_score)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-05"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_outcome.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
